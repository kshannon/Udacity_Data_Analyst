{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kyle Shannon 1/18/16 - Project 5 - Enron POI Predictive Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files:\n",
    "\n",
    "**poi_id.py:** Main file where I construct my pipeline classifier: MinMax/Scaler/PCA/Classifiers etc.\n",
    "\n",
    "**data_shape.py:** python script to remove outliers, print out useful info about data set and add features.\n",
    "\n",
    "**data_viz.py:** python script taking a data_dict and trnasforming it into a Pandas DF. Also has several graphing functions to show correlation plots and pairplots. Useful for identifying feature interactions.\n",
    "\n",
    "**tester.py:** file generated by Udacity to test out algorithm and print out useful information about performance.\n",
    "\n",
    "**three pickle files:** files generated by poi_id.py which tester.py uses. The pickle files are essentially my classifier or pipeline along with the transformed data set and feature list.\n",
    "\n",
    "**references.txt:** text file with list of refernces I used.\n",
    "\n",
    "**ml_results.txt:** I piped the output of tester.py to this file so I could save all of my ML algorithm attempts. This is essentially a record of all my attempts picking an algorithm and trying to fine tune it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Dataset and Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Exploration & Outlier Investigation\n",
    "\n",
    "I started out with basic exploratory methods (printing out types, counts, percentages and sample graphing). I noticed several, what could be considered, outliers. Individuals such as Lay and Skilling could be considered outliers; however, because they are POIs and due to severe class imbalances they could not be removed. There were three outliers I did remove:\n",
    "\t\n",
    "    del data_dict['TOTAL'] # this is not a person and should be removed.\n",
    "\tdel data_dict['THE TRAVEL AGENCY IN THE PARK'] # not identified as a POI and had little financial data.\n",
    "\tdel data_dict['LOCKHART EUGENE E'] # All data points were NaN and he was not a POI.\n",
    "    \n",
    "Other interesting information I found included:\n",
    "\n",
    "- Number of People under Investigation: 143\n",
    "- Number of Data Points: 3718 (this was after outliers were removed and I added new features, derived by: num_of_people * num_of_features)\n",
    "- Number of Features: 26 (I added new features)\n",
    "- Num of POIs:  18\n",
    "- Percentage of data points as NaNs: 35%\n",
    "\n",
    "I also created a new dict that provided a keys as features and value as % of NaNs in data set: \n",
    "\n",
    "    {'to_messages': '39.86%', 'deferral_payments': '73.43%', 'expenses': '34.27%', 'poi_email_reciept_interaction': '0.00%', 'poi': '0.00%', 'deferred_income': '66.43%', 'email_address': '22.38%', 'from_poi_to_this_person': '39.86%', 'restricted_stock_deferred': '88.11%', 'shared_receipt_with_poi': '39.86%', 'loan_advances': '97.90%', 'from_messages': '39.86%', 'other': '36.36%', 'from_this_person_to_poi_fraction': '0.00%', 'director_fees': '88.81%', 'salary': '34.27%', 'bonus': '43.36%', 'total_stock_value': '12.59%', 'poi_email_interaction': '0.00%', 'from_this_person_to_poi': '39.86%', 'restricted_stock': '23.78%', 'adj_compensation': '0.00%', 'total_payments': '13.99%', 'long_term_incentive': '54.55%', 'from_poi_to_this_person_fraction': '0.00%', 'exercised_stock_options': '29.37%'}\n",
    "\n",
    "A lot of the email features have little to no NaNs, most of the NaNs are in the financial data, especially 'restricted_stock_deferred', 'loan_advances', and 'director_fees'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize Feature Selection/Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first I thought about ways to imputate data for NaNs. For example using regression to predict values. However, it hit me as I looked over the financial PDF. NaNs should be Zeros, due to no financial data, if 'salary' was NaN that was because they did not receive a salary, and I should not imputate a salary for that person."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created 5 new features:\n",
    "        \n",
    "1. **'poi_email_interaction'** - People who received emails from POIs probably also responded to POIs and vice versa. I decided to combine emails (from POIs) and (to POIs) into one POI interaction.\n",
    "\n",
    "2. **'poi_email_reciept_interaction'** - This feature takes poi_email_interaction and multiplies the values by the number of receipts shared with POIs. I thought that it was easier to share an email with a POI vs a receipt. The more receipts a person has, the more the personal the relationship. Therefore there is a potential greater chance that person in a POIS as well. \n",
    "\n",
    "3. **'adj_compensation'** - I created this feature by combining financial features that might make up an employee's total compensation from Enron. I added: 'salary', 'total_payments', 'exercised_stock_options', 'bonus', 'long_term_incentive', and 'total_stock_value'.\n",
    "\n",
    "4. **'from_poi_to_this_person_fraction'** - This feature and the following one are the respective  fraction of poi emails from total emails. This feature will tease out individuals who send or receive less emails, however a majority of their emails are interactions with POIs.\n",
    "\n",
    "5. **'from_this_person_to_poi_fraction'** - See the previous feature description above.\n",
    "\n",
    "Example of code used to create the 'adj_compensation' feature:\n",
    "        \n",
    "        if (key == 'salary' or key == 'total_payments' or key == 'exercised_stock_options' \\\n",
    "\t\t\t\tor key == 'bonus' or key == 'long_term_incentive' or key == 'total_stock_value') \\\n",
    "\t\t\t\tand value != 'NaN':\n",
    "\t\t\t\tv['adj_compensation'] += value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intelligently select features \n",
    "\n",
    "I tried manually selecting features, based on evidence supplied by a Correlation Matrix Heatmap (see figure 1). I looked at features that were highly correlated with many other features and choose not to use those. I wanted features that were mostly uncorrelated as they themselves explained a lot of the variance. Highly correlated features can create imbalances in the weight distributions in hyperplanes where algorithms look to classify data. Many features that overlap in the same space will create unneeded bias. On the other hand this imbalance can overshadow significant features and make them look insignificant. What a shame. So we want to limit collinearity in our model. \n",
    "\n",
    "<img src=\"figure_1.png\" alt=\"inline\" style=\"width: 800px;\">\n",
    "\n",
    "I tried adding SelectKBest to my pipeline to help computationally determine features to use. I attempted to use several parameters using GridSearch, for example (f_classif, and k=[2,4,6,8,10]), however, I could not get the precision and recall score both above 3.0 keeping everything else equal, compared to manually selecting features. The features I manually chose were:\n",
    "\n",
    "    features_list = [\"poi\", #'poi' must be 1st feature in list\n",
    "                    \"salary\",\n",
    "                    \"total_payments\",\n",
    "                    \"exercised_stock_options\",\n",
    "                    \"restricted_stock\",\n",
    "                    \"expenses\",\n",
    "                    \"director_fees\",\n",
    "                    \"deferred_income\",\n",
    "                    \"from_poi_to_this_person_fraction\",\n",
    "                    \"from_this_person_to_poi_fraction\",\n",
    "                    \"poi_email_reciept_interaction\"]\n",
    "\n",
    "This above list was selected after looking at the heatmap correlation matrix and many pairplots.\n",
    "\n",
    "The below function and list shows what SelectKBest choose as the best features.\n",
    "\n",
    "    def feature_select(clf, feature, label):\n",
    "        '''\n",
    "        Args:\n",
    "            1. clf: select a classifier that selects best features.\n",
    "            2. feature: features from data set.\n",
    "            3. label: classification labels.\n",
    "\n",
    "        Function that selects the best features to use and prints all the features \n",
    "        with scores and a boolean value if the feature should be used. \n",
    "        '''\n",
    "\n",
    "        features_new = clf.fit(feature, label)\n",
    "        SKB_scores =  features_new.scores_\n",
    "        SKB_get_support =  features_new.get_support()\n",
    "\n",
    "\n",
    "        best_features = []\n",
    "        for foo in range(0, len(features_list)-1):\n",
    "            temp_tuple = (features_list[foo + 1], SKB_scores[foo], SKB_get_support[foo])\n",
    "            best_features.append(temp_tuple)\n",
    "\n",
    "        sorted_best_features = sorted(best_features, key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "        for tup in sorted_best_features:\n",
    "            print tup\n",
    "\n",
    "    ### Use SelectKBest to select the 10 best features.\n",
    "    feature_select(SelectKBest(f_classif), features, labels)\n",
    "\n",
    "    ('exercised_stock_options', 24.815079733218194, True)\n",
    "    ('total_stock_value', 24.182898678566879, True)\n",
    "    ('adj_compensation', 22.292299851749735, True)\n",
    "    ('bonus', 20.792252047181535, True)\n",
    "    ('salary', 18.289684043404513, True)\n",
    "    ('from_this_person_to_poi_fraction', 16.409712548035792, True)\n",
    "    ('deferred_income', 11.458476579280369, True)\n",
    "    ('long_term_incentive', 9.9221860131898225, True)\n",
    "    ('restricted_stock', 9.2128106219771002, True)\n",
    "    ('total_payments', 8.7727777300916756, True) \n",
    "    ('shared_receipt_with_poi', 8.589420731682381, False)\n",
    "    ('loan_advances', 7.1840556582887247, False)\n",
    "    ('expenses', 6.0941733106389453, False)\n",
    "    ('from_poi_to_this_person', 5.2434497133749582, False)\n",
    "    ('poi_email_reciept_interaction', 4.8964971483426085, False)\n",
    "    ('poi_email_interaction', 4.8636818394122443, False)\n",
    "    ('from_poi_to_this_person_fraction', 3.1280917481567192, False)\n",
    "    ('from_this_person_to_poi', 2.3826121082276739, False)\n",
    "    ('director_fees', 2.1263278020077054, False)\n",
    "    ('to_messages', 1.6463411294420076, False)\n",
    "    ('deferral_payments', 0.22461127473600989, False)\n",
    "    ('from_messages', 0.16970094762175533, False)\n",
    "    ('restricted_stock_deferred', 0.065499652909942141, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Properly Scale Features\n",
    "\n",
    "I was going to scale features at the beginning of my pipeline, using either: MinMaxScaler() or \n",
    "StandardScaler(). MinMaxScaler lets you choose a range to scale all features to, for example between [0,1]. Whereas StandardScaler normalizes the data between [-1, 1] with a mean 0. I believe StandardScaler is better for certain methods, e.g. PCA. On the flip side if a tree based method, such as Decision Tree is selected, then no feature scaling should be necessary.\n",
    "\n",
    "One area of concern I had was the following. My pipeline:\n",
    "\n",
    "    decision_tree_pipeline = Pipeline([\n",
    "                ('pca', PCA()),\n",
    "                ('dt', DecisionTreeClassifier(max_depth=10))\n",
    "                ])\n",
    "\n",
    "I achieved:\n",
    "\n",
    "    Accuracy: 0.82807\t Precision: 0.35575\tRecall: 0.35700\tF1: 0.35638\n",
    "    \n",
    "However when I added:\n",
    "\n",
    "     decision_tree_pipeline = Pipeline([\n",
    "\t    (‘scaler’, StandardScaler()),\n",
    "        ('pca', PCA()),\n",
    "        ('dt', DecisionTreeClassifier(max_depth=10))\n",
    "        ])\n",
    "\n",
    "I received a score of: (I got a similar score when I used MinMaxScaler() as well)\n",
    "\n",
    "    Accuracy: 0.80707\t Precision: 0.27053\tRecall: 0.26350\tF1: 0.26697\n",
    "\n",
    "I was not sure why the score went down so much. I assumed PCA would perform better with data that was standardized. Perhaps this is because I am using a decision tree, or maybe it has to do with the parameter tuning of ‘scaler’ or ‘pca’."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick and Tune an Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try Out Several Algorithms\n",
    "\n",
    "I tried several 'out of the box' algorithms:\n",
    "\n",
    "    # DT out of box\n",
    "    test_pipeline_dt = Pipeline([\t\n",
    "                ('select', SelectKBest(k=3)),\n",
    "                ('dt', DecisionTreeClassifier()),\n",
    "                ])\n",
    "    clf = test_pipeline_dt\n",
    "    # \tAccuracy: 0.81260\tPrecision: 0.30663\tRecall: 0.32150\tF1: 0.31389\tF2: 0.31841\n",
    "\n",
    "\n",
    "    # GussianNB out of box\n",
    "    test_pipeline_gnb = Pipeline([\t\n",
    "                ('select', SelectKBest(k=3)),\n",
    "                ('gnb', GaussianNB())\n",
    "                ])\n",
    "    clf = test_pipeline_gnb\n",
    "    # Accuracy: 0.84147\tPrecision: 0.35462\tRecall: 0.23050\tF1: 0.27939\tF2: 0.24785\n",
    "\n",
    "    # AdaBoost out of box\n",
    "    test_pipeline_adaboost = Pipeline([\t\n",
    "                ('select', SelectKBest(k=3)),\n",
    "                ('adaboost', AdaBoostClassifier())\n",
    "                ])\n",
    "    clf = test_pipeline_adaboost\n",
    "    # Accuracy: 0.83047\tPrecision: 0.33535\tRecall: 0.27650\tF1: 0.30310\tF2: 0.28656\n",
    "\n",
    "    # Linear SVC out of box\n",
    "    test_pipeline_Lsvc = Pipeline([\t\n",
    "                ('select', SelectKBest(k=3)),\n",
    "                ('Lsvc', svm.LinearSVC())\n",
    "                ])\n",
    "    clf = test_pipeline_Lsvc\n",
    "    # Accuracy: 0.67860\tPrecision: 0.10874\tRecall: 0.19600\tF1: 0.13988\tF2: 0.16889\n",
    "\n",
    "    # Random Forest out of box\n",
    "    test_pipeline_rf = Pipeline([\t\n",
    "                ('select', SelectKBest(k=3)),\n",
    "                ('rf', RandomForestClassifier(n_estimators=10))\n",
    "                ])\n",
    "    clf = test_pipeline_rf\n",
    "    # Accuracy: 0.85147\tPrecision: 0.38000\tRecall: 0.18050\tF1: 0.24475\tF2: 0.20168"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pick an algorithm \n",
    "\n",
    "Going off F1 scores, decision trees seemed to work best as a baseline model. Because of this, I decided to forge down the Decision Tree path and begin to tune the model using PCA, GridSearch and playing with the features a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune an algorithm\n",
    "\n",
    "\n",
    "I had an issue  trying to tune PCA. When using all default parameters I recieved precision and recall scores above 3.0. However, when I tried cycling through n_components in GridSearch(keeping everything else equal):\n",
    "\n",
    "    param_grid={'pca__n_components': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}\n",
    "\n",
    "I received scores of:\n",
    "\n",
    "    Accuracy: 0.80527\tPrecision: 0.25750\tRecall: 0.24450\tF1: 0.25083\n",
    "\n",
    "Using the same feature list as above. I was not sure why the scores would go down. At least they should stay the same I would think.\n",
    "\n",
    "A parameter of Decision Tree that I wanted to explore was 'max_depth'. Allowing a tree to go as deep as it possibly can cause overfitting. Forcing it to stop earlier probably lowers accuracy; however, it is a reasonable tradeoff. In this case we prefer to gain some bias to lose a bit of variance. I attempted to use grid search with \n",
    "\n",
    "    {'dt__max_depth' : range(12)} \n",
    "\n",
    "GridSearchCV choose a ‘max_depth’ of None. Giving me a score of\n",
    "\n",
    "    Accuracy: 0.83807\tPrecision: 0.32080\tRecall: 0.19200\tF1: 0.24023\t\n",
    "    \n",
    "However, when I manually tried out numbers I got a score of\n",
    "\n",
    "    Accuracy: 0.82807\tPrecision: 0.35575\tRecall: 0.35700\tF1: 0.35638\t\n",
    "    \n",
    "Using ‘max_depth’ = 10. Again I am not sure why GridSearch did not work here. As I kept everything else constant.\n",
    "\n",
    "This was the best decision tree model I was able to come up with. I winded up manually testing out the parameters, as I could not get GridSearch to behave. Perhaps I wrongly thought I could pass pipelines and GridSearchCV as a clf to the tester.py file.\n",
    "\n",
    "    ['poi', 'salary', 'total_payments', 'exercised_stock_options', 'restricted_stock', 'expenses', 'director_fees', 'deferred_income', 'from_poi_to_this_person_fraction', 'from_this_person_to_poi_fraction', 'poi_email_reciept_interaction']\n",
    "    \n",
    "    GridSearchCV(cv=None, error_score='raise',\n",
    "           estimator=Pipeline(steps=[('pca', PCA(copy=True, n_components=None, whiten=False)), ('dt', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
    "                max_features=None, max_leaf_nodes=None, min_samples_leaf=3,\n",
    "                min_samples_split=10, min_weight_fraction_leaf=0.0,\n",
    "                presort=False, random_state=None, splitter='best'))]),\n",
    "           fit_params={}, iid=True, n_jobs=1, param_grid={},\n",
    "           pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)\n",
    "        \n",
    "        Accuracy: 0.84427\tPrecision: 0.40289\tRecall: 0.34850\tF1: 0.37373\tF2: 0.35817\n",
    "        Total predictions: 15000\tTrue positives:  697\tFalse positives: 1033\tFalse negatives: 1303\tTrue negatives: 11967\n",
    "\n",
    "Tuning a model is important because the parameters you feed into classifiers (SelectKBest, PCA, DecisionTree etc.) will all affect how your final classifier generalizes on unseen data. Inorder to tune the algorithm we have at our disposal methods such as Cross Validation and GidSearchCV. Usually a good rule of thumb is to split your data randomly into train, validate and test sets. The train set is to fit your model, the validate set helps you to test/tune your algorithm and the test set is unseen data that the algorithm has not yet seen. This test tells you how well your algorithm generaizes to unseen data. This process can be set up in many ways using SciKitLearn's Cross Validation Tools. \n",
    "\n",
    "GirdSearchCV allows us to try out many different versions of our algorithm. For example PCA has a parameter called n_components = (int). Where as int is some number (1, 2, 3 ... n). I would not want to change that number and re run my algorithm again and keep track of the score. I can use GridSearch to perform this tedious task for me. For example I would pass { pca__n_components : [ 2, 4, 6 ] }. Then GridSearchCV would create a grid and construct a PCA classifier with n_components for 2, 4, and 6. It would then run all three PCAs and report back to me which parameter performed best. Furthermore you can extend this by passing GridSearchCV a pipeline of many classifiers and steps. E.g. SelectKBest, PCA, DecisionTree and try out different parameters for all of them. Though it may take some time. \n",
    "\n",
    "For this project I decided to not do CrossValidation by itself, beacuse the data set was heavily class imbalanced and there was just not a lot of data. What I did do is use GridSearchCV and I changed a parameter in in grid search, 'cv': \n",
    "\n",
    "    cv = StratifiedShuffleSplit(labels_df, folds, random_state)\n",
    "    clf = GridSearchCV(estimator, param_dict, cv=cv)\n",
    "\n",
    "So that I am at least performing cross validation withn grid search while the parameters are being tuned. StratifiedShuffleSplit is one form cross validation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate and Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usage of Evaluation Metrics\n",
    "\n",
    "While accuracy can be a good predictor, it is not the only evaluation criteria we can use. Precision, Recall and F1 scores are also available. Precision and Recall are especially well suited for this data set. In our data there is a sever class imbalance (most data can be classified as non-POI). If we relied only on accuracy then our model could simply select not-POI for every prediction and return a fairly good accuracy. We would never know that no POIs were being correctly classified. Precision and Recall tells us how many POIs were correctly and incorrectly classified. Furthermore, we can tune our model to favor one over the other or attempt to strike a balance between the two.\n",
    "\n",
    "Precision can be thought of as **exactness**. Specifically a high score tells us that we have less false positives. Specifically precision in this context is the proportion of correctly identified POIs over the value of correctly identified POIs combined with Innocent people mislabeled as POIs. For example if you identified 5 POIs correctly and 5 non-POIs as POIs then you would have a precision of 50%. \n",
    "\n",
    "##### *Precision* = (*TP*) / (*TP* + *FP*)\n",
    "\n",
    "Where:\n",
    "- TP = True Positive\n",
    "- FP = False Positive\n",
    "\n",
    "Recall can be thought of as **completness**. Specifically a high Recall score shows us that we have less false negatives. Recall in this context tells us the percentage of POIs not identified as a POI. In other words our classifier missed them. E.g. if we correctly identified 3 POIs, but missed 1 POI, then our recall would be 75%. \n",
    "\n",
    "##### *Recall* = (*TP*) / (*TP* + *FN*)\n",
    "\n",
    "Where:\n",
    "- TP = True Positive\n",
    "- FN = False Negative\n",
    "\n",
    "***F1 Scores*** provide you with a value that determines how well recall and precision are balanced. E.g. if recall was very high and precision was very low then the F1 scre would be about in the middle.  \n",
    "\n",
    "\n",
    "For this prediction algorithm I would argue that we care more about Recall. Because were are concerned with identifying POIs for further investigation. Therefore we rather get all POIs even if that means investigating some innocent people. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Strategy\n",
    "\n",
    "\n",
    "Validation is very important in machine learning. For example validating a classifier can reduce over fitting on never before seen data, ensuring generalizability. This mindset goes back to the bias-variance tradeoff. Models may be more biased (think linear regression) or varianced (think SVM). Often an overly biased model  does not meet the complexities of the real world and we thus underfit. On the flip side a high variance model that overfits the data may produce great results, but in the real world will often be lackluster. Thus a balance is needed. Methods like cross validation and all of its implementations helps alleviate this issue, but employing various training, validating and testing sets to ensure models train on one set of data and are tested on a pure untouched set of data as well, usually smaller than the fit. \n",
    "\n",
    "This methodology can run into issues though when your data set is not very large, like the Enron set. Or classes are imbalanced, like in the Enron set (~18 POIs vs. oer a hundered non-POIs). To help remedy these issues we can employ strategies such as StratefiedShuffeSplit to partition our dataset and test, without having to permanently partition a training and testing set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm Performance\n",
    "\n",
    "The score for my final algorithm was:\n",
    "\n",
    "    Accuracy: 0.84427\tPrecision: 0.40289\tRecall: 0.34850\tF1: 0.37373\n",
    "    True positives:  697\tFalse positives: 1033\tFalse negatives: 1303\tTrue negatives: 11967\n",
    "    \n",
    "The algorithm was:\n",
    "\n",
    "    GridSearchCV(cv=None, error_score='raise',\n",
    "           estimator=Pipeline(steps=[('pca', PCA(copy=True, n_components=None, whiten=False)), ('dt', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
    "                max_features=None, max_leaf_nodes=None, min_samples_leaf=3,\n",
    "                min_samples_split=10, min_weight_fraction_leaf=0.0,\n",
    "                presort=False, random_state=None, splitter='best'))]),\n",
    "           fit_params={}, iid=True, n_jobs=1, param_grid={},\n",
    "           pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)\n",
    "           \n",
    "And feature list:\n",
    "\n",
    "    ['poi', 'salary', 'total_payments', 'exercised_stock_options', 'restricted_stock', 'expenses', 'director_fees', 'deferred_income', 'from_poi_to_this_person_fraction', 'from_this_person_to_poi_fraction', 'poi_email_reciept_interaction']\n",
    "\n",
    "I was able to get a decent precision score; however, I had trouble getting recall past the .40 mark. Overall I found it diffcult to impment feature selecting with SelectKBest and StandardScaler() with PCA. Attempting to reproduce results with a pipeline() and GridSearchCV() also gave me some trouble. However, I was able to produce a simple model eith precision and recall both above .30 using manual manipulation techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. https://www.oreilly.com/learning/handling-missing-data imputation\n",
    "2. http://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues PCA\n",
    "3. Python Machine Learning by Raschka\n",
    "4. ISL by Hastie et al.\n",
    "5. https://discussions.udacity.com/t/using-pipeline-precision-recall-have-been-decreased/45992/5\n",
    "6. SkLearn documentation\n",
    "7. PANDAS documentation\n",
    "8. Seaborn documentation\n",
    "9. MatPlotLib documentation\n",
    "10. http://stats.stackexchange.com/questions/53/pca-on-correlation-or-covariance\n",
    "11. Many more stack exachage, cross validated, and udacity forum posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
