{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kyle Shannon 1/18/16 - Project 5 - Enron POI Predictive Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files:\n",
    "\n",
    "**poi_id.py:** Main file where I construct my pipeline classifier: MinMax/Scaler/PCA/Classifiers etc.\n",
    "\n",
    "**data_shape.py:** python script to remove outliers, print out useful info about data set and add features.\n",
    "\n",
    "**data_viz.py:** python script taking a data_dict and trnasforming it into a Pandas DF. Also has several graphing functions to show correlation plots and pairplots. Useful for identifying feature interactions.\n",
    "\n",
    "**tester.py:** file generated by Udacity to test out algorithm and print out useful information about performance.\n",
    "\n",
    "**dummy_transform.py:** Class to be used within pipelines so I can set a pipeline stage essentially qual to None. This is just a quick way to test things out, though after writing it I felt like it was a bit of a wasted effort.\n",
    "\n",
    "**three pickle files:** files generated by poi_id.py which tester.py uses. The pickle files are essentially my classifier or pipeline along with the transformed data set and feature list.\n",
    "\n",
    "**references.txt:** text file with list of refernces I used.\n",
    "\n",
    "**ml_results.txt:** I piped the output of tester.py to this file so I could save all of my ML algorithm attempts. This is essentially a record of all my attempts picking an algorithm and trying to fine tune it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Dataset and Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Exploration & Outlier Investigation\n",
    "\n",
    "I started out with basic exploratory methods (printing out types, counts, percentages and sample graphing). I noticed several, what could be considered, outliers. Individuals such as Lay and Skilling could be considered outliers; however, because they are POIs and due to severe class imbalances they could not be removed. There were three outliers I did remove:\n",
    "\t\n",
    "    del data_dict['TOTAL'] # this is not a person and should be removed.\n",
    "\tdel data_dict['THE TRAVEL AGENCY IN THE PARK'] # not identified as a POI and had little financial data.\n",
    "\tdel data_dict['LOCKHART EUGENE E'] # All data points were NaN and he was not a POI.\n",
    "    \n",
    "Other interesting information I found included:\n",
    "\n",
    "- Number of People under Investigation: 146\n",
    "- Number of Data Points or observations used: 143 (this was after outliers/incorrect data were removed)\n",
    "- Number of Features: 26 (I added new features)\n",
    "- Num of POIs:  18\n",
    "- Percentage of data points as NaNs: 35%\n",
    "\n",
    "I also created a new dict that provided a keys as features and value as % of NaNs in data set: \n",
    "\n",
    "    {'to_messages': '39.86%', 'deferral_payments': '73.43%', 'expenses': '34.27%', 'poi_email_reciept_interaction': '0.00%', 'poi': '0.00%', 'deferred_income': '66.43%', 'email_address': '22.38%', 'from_poi_to_this_person': '39.86%', 'restricted_stock_deferred': '88.11%', 'shared_receipt_with_poi': '39.86%', 'loan_advances': '97.90%', 'from_messages': '39.86%', 'other': '36.36%', 'from_this_person_to_poi_fraction': '0.00%', 'director_fees': '88.81%', 'salary': '34.27%', 'bonus': '43.36%', 'total_stock_value': '12.59%', 'poi_email_interaction': '0.00%', 'from_this_person_to_poi': '39.86%', 'restricted_stock': '23.78%', 'adj_compensation': '0.00%', 'total_payments': '13.99%', 'long_term_incentive': '54.55%', 'from_poi_to_this_person_fraction': '0.00%', 'exercised_stock_options': '29.37%'}\n",
    "\n",
    "A lot of the email features have little to no NaNs, most of the NaNs are in the financial data, especially 'restricted_stock_deferred', 'loan_advances', and 'director_fees'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize Feature Selection/Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first I thought about ways to imputate data for NaNs. For example using regression to predict values. However, it hit me as I looked over the financial PDF. NaNs should be Zeros, due to no financial data, if 'salary' was NaN that was because they did not receive a salary, and I should not imputate a salary for that person."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created 5 new features:\n",
    "        \n",
    "1. **'poi_email_interaction'** - People who received emails from POIs probably also responded to POIs and vice versa. I decided to combine emails (from POIs) and (to POIs) into one POI interaction.\n",
    "\n",
    "2. **'poi_email_reciept_interaction'** - This feature takes poi_email_interaction and multiplies the values by the number of receipts shared with POIs. I thought that it was easier to share an email with a POI vs a receipt. The more receipts a person has, the more the personal the relationship. Therefore there is a potential greater chance that person in a POIS as well. \n",
    "\n",
    "3. **'adj_compensation'** - I created this feature by combining financial features that might make up an employee's total compensation from Enron. I added: 'salary', 'total_payments', 'exercised_stock_options', 'bonus', 'long_term_incentive', and 'total_stock_value'.\n",
    "\n",
    "4. **'from_poi_to_this_person_fraction'** - This feature and the following one are the respective  fraction of poi emails from total emails. This feature will tease out individuals who send or receive less emails, however a majority of their emails are interactions with POIs.\n",
    "\n",
    "5. **'from_this_person_to_poi_fraction'** - See the previous feature description above.\n",
    "\n",
    "Example of code used to create the 'adj_compensation' feature:\n",
    "        \n",
    "        if (key == 'salary' or key == 'total_payments' or key == 'exercised_stock_options' \\\n",
    "\t\t\t\tor key == 'bonus' or key == 'long_term_incentive' or key == 'total_stock_value') \\\n",
    "\t\t\t\tand value != 'NaN':\n",
    "\t\t\t\tv['adj_compensation'] += value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intelligently select features \n",
    "\n",
    "I tried manually selecting features, based on evidence supplied by a Correlation Matrix Heatmap (see figure 1). I looked at features that were highly correlated with many other features and choose not to use those. I wanted features that were mostly uncorrelated as they themselves explained a lot of the variance. Highly correlated features can create imbalances in the weight distributions in hyperplanes where algorithms look to classify data. Many features that overlap in the same space will create unneeded bias. On the other hand this imbalance can overshadow significant features and make them look insignificant. What a shame. So we want to limit collinearity in our model. \n",
    "\n",
    "<img src=\"figure_1.png\" alt=\"inline\" style=\"width: 800px;\">\n",
    "\n",
    "I tried adding SelectKBest to my pipeline to help computationally determine features to use. I attempted to use several parameters using GridSearch, for example (f_classif, and k=[2,4,6,8,10]), however, I could not get the precision and recall score both above 3.0 keeping everything else equal, compared to manually selecting features. The features I manually chose were:\n",
    "\n",
    "    features_list = [\"poi\", #'poi' must be 1st feature in list\n",
    "                    \"salary\",\n",
    "                    \"total_payments\",\n",
    "                    \"exercised_stock_options\",\n",
    "                    \"restricted_stock\",\n",
    "                    \"expenses\",\n",
    "                    \"director_fees\",\n",
    "                    \"deferred_income\",\n",
    "                    \"from_poi_to_this_person_fraction\",\n",
    "                    \"from_this_person_to_poi_fraction\",\n",
    "                    \"poi_email_reciept_interaction\"]\n",
    "\n",
    "This above list was selected after looking at the heatmap correlation matrix and many pairplots.\n",
    "\n",
    "The below function and list shows what SelectKBest choose as the best features.\n",
    "\n",
    "    def feature_select(clf, feature, label):\n",
    "        '''\n",
    "        Args:\n",
    "            1. clf: select a classifier that selects best features.\n",
    "            2. feature: features from data set.\n",
    "            3. label: classification labels.\n",
    "\n",
    "        Function that selects the best features to use and prints all the features \n",
    "        with scores and a boolean value if the feature should be used. \n",
    "        '''\n",
    "\n",
    "        features_new = clf.fit(feature, label)\n",
    "        SKB_scores =  features_new.scores_\n",
    "        SKB_get_support =  features_new.get_support()\n",
    "\n",
    "\n",
    "        best_features = []\n",
    "        for foo in range(0, len(features_list)-1):\n",
    "            temp_tuple = (features_list[foo + 1], SKB_scores[foo], SKB_get_support[foo])\n",
    "            best_features.append(temp_tuple)\n",
    "\n",
    "        sorted_best_features = sorted(best_features, key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "        for tup in sorted_best_features:\n",
    "            print tup\n",
    "\n",
    "    ### Use SelectKBest to select the 10 best features.\n",
    "    feature_select(SelectKBest(f_classif), features, labels)\n",
    "\n",
    "    ('exercised_stock_options', 24.815079733218194, True)\n",
    "    ('total_stock_value', 24.182898678566879, True)\n",
    "    ('adj_compensation', 22.292299851749735, True)\n",
    "    ('bonus', 20.792252047181535, True)\n",
    "    ('salary', 18.289684043404513, True)\n",
    "    ('from_this_person_to_poi_fraction', 16.409712548035792, True)\n",
    "    ('deferred_income', 11.458476579280369, True)\n",
    "    ('long_term_incentive', 9.9221860131898225, True)\n",
    "    ('restricted_stock', 9.2128106219771002, True)\n",
    "    ('total_payments', 8.7727777300916756, True) \n",
    "    ('shared_receipt_with_poi', 8.589420731682381, False)\n",
    "    ('loan_advances', 7.1840556582887247, False)\n",
    "    ('expenses', 6.0941733106389453, False)\n",
    "    ('from_poi_to_this_person', 5.2434497133749582, False)\n",
    "    ('poi_email_reciept_interaction', 4.8964971483426085, False)\n",
    "    ('poi_email_interaction', 4.8636818394122443, False)\n",
    "    ('from_poi_to_this_person_fraction', 3.1280917481567192, False)\n",
    "    ('from_this_person_to_poi', 2.3826121082276739, False)\n",
    "    ('director_fees', 2.1263278020077054, False)\n",
    "    ('to_messages', 1.6463411294420076, False)\n",
    "    ('deferral_payments', 0.22461127473600989, False)\n",
    "    ('from_messages', 0.16970094762175533, False)\n",
    "    ('restricted_stock_deferred', 0.065499652909942141, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Properly Scale Features\n",
    "\n",
    "I was going to scale features at the beginning of my pipeline, using either: MinMaxScaler() or \n",
    "StandardScaler(). MinMaxScaler lets you choose a range to scale all features to, for example between [0,1]. Whereas StandardScaler normalizes the data between [-1, 1] with a mean 0. I believe StandardScaler is better for certain methods, e.g. PCA. On the flip side if a tree based method, such as Decision Tree is selected, then no feature scaling should be necessary.\n",
    "\n",
    "One area of concern I had was the following. My pipeline:\n",
    "\n",
    "    decision_tree_pipeline = Pipeline([\n",
    "                ('pca', PCA()),\n",
    "                ('dt', DecisionTreeClassifier(max_depth=10))\n",
    "                ])\n",
    "\n",
    "I achieved:\n",
    "\n",
    "    Accuracy: 0.82807\t Precision: 0.35575\tRecall: 0.35700\tF1: 0.35638\n",
    "    \n",
    "However when I added:\n",
    "\n",
    "     decision_tree_pipeline = Pipeline([\n",
    "\t    (‘scaler’, StandardScaler()),\n",
    "        ('pca', PCA()),\n",
    "        ('dt', DecisionTreeClassifier(max_depth=10))\n",
    "        ])\n",
    "\n",
    "I received a score of: (I got a similar score when I used MinMaxScaler() as well)\n",
    "\n",
    "    Accuracy: 0.80707\t Precision: 0.27053\tRecall: 0.26350\tF1: 0.26697\n",
    "\n",
    "I was not sure why the score went down so much. I assumed PCA would perform better with data that was standardized. Perhaps this is because I am using a decision tree, or maybe it has to do with the parameter tuning of ‘scaler’ or ‘pca’."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick and Tune an Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try Out Several Algorithms\n",
    "\n",
    "I tried several 'out of the box' algorithms:\n",
    "\n",
    "    # DT out of box\n",
    "    test_pipeline_dt = Pipeline([\t\n",
    "                ('select', SelectKBest(k=3)),\n",
    "                ('dt', DecisionTreeClassifier()),\n",
    "                ])\n",
    "    clf = test_pipeline_dt\n",
    "    # \tAccuracy: 0.81260\tPrecision: 0.30663\tRecall: 0.32150\tF1: 0.31389\tF2: 0.31841\n",
    "\n",
    "\n",
    "    # GussianNB out of box\n",
    "    test_pipeline_gnb = Pipeline([\t\n",
    "                ('select', SelectKBest(k=3)),\n",
    "                ('gnb', GaussianNB())\n",
    "                ])\n",
    "    clf = test_pipeline_gnb\n",
    "    # Accuracy: 0.84147\tPrecision: 0.35462\tRecall: 0.23050\tF1: 0.27939\tF2: 0.24785\n",
    "\n",
    "    # AdaBoost out of box\n",
    "    test_pipeline_adaboost = Pipeline([\t\n",
    "                ('select', SelectKBest(k=3)),\n",
    "                ('adaboost', AdaBoostClassifier())\n",
    "                ])\n",
    "    clf = test_pipeline_adaboost\n",
    "    # Accuracy: 0.83047\tPrecision: 0.33535\tRecall: 0.27650\tF1: 0.30310\tF2: 0.28656\n",
    "\n",
    "    # Linear SVC out of box\n",
    "    test_pipeline_Lsvc = Pipeline([\t\n",
    "                ('select', SelectKBest(k=3)),\n",
    "                ('Lsvc', svm.LinearSVC())\n",
    "                ])\n",
    "    clf = test_pipeline_Lsvc\n",
    "    # Accuracy: 0.67860\tPrecision: 0.10874\tRecall: 0.19600\tF1: 0.13988\tF2: 0.16889\n",
    "\n",
    "    # Random Forest out of box\n",
    "    test_pipeline_rf = Pipeline([\t\n",
    "                ('select', SelectKBest(k=3)),\n",
    "                ('rf', RandomForestClassifier(n_estimators=10))\n",
    "                ])\n",
    "    clf = test_pipeline_rf\n",
    "    # Accuracy: 0.85147\tPrecision: 0.38000\tRecall: 0.18050\tF1: 0.24475\tF2: 0.20168"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pick an algorithm \n",
    "\n",
    "Going off F1 scores, decision trees seemed to work best as a baseline model. Because of this, I decided to forge down the Decision Tree path and begin to tune the model using PCA, GridSearch and playing with the features a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune an algorithm\n",
    "\n",
    "\n",
    "I had an issue  trying to tune PCA. When using all default parameters I recieved precision and recall scores above 3.0. However, when I tried cycling through n_components in GridSearch(keeping everything else equal):\n",
    "\n",
    "    param_grid={'pca__n_components': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}\n",
    "\n",
    "I received scores of:\n",
    "\n",
    "    Accuracy: 0.80527\tPrecision: 0.25750\tRecall: 0.24450\tF1: 0.25083\n",
    "\n",
    "Using the same feature list as above. I was not sure why the scores would go down. At least they should stay the same I would think.\n",
    "\n",
    "A parameter of Decision Tree that I wanted to explore was 'max_depth'. Allowing a tree to go as deep as it possibly can cause overfitting. Forcing it to stop earlier probably lowers accuracy; however, it is a reasonable tradeoff. In this case we prefer to gain some bias to lose a bit of variance. I attempted to use grid search with \n",
    "\n",
    "    {'dt__max_depth' : range(12)} \n",
    "\n",
    "GridSearchCV choose a ‘max_depth’ of None. Giving me a score of\n",
    "\n",
    "    Accuracy: 0.83807\tPrecision: 0.32080\tRecall: 0.19200\tF1: 0.24023\t\n",
    "    \n",
    "However, when I manually tried out numbers I got a score of\n",
    "\n",
    "    Accuracy: 0.82807\tPrecision: 0.35575\tRecall: 0.35700\tF1: 0.35638\t\n",
    "    \n",
    "Using ‘max_depth’ = 10. Again I am not sure why GridSearch did not work here. As I kept everything else constant.\n",
    "\n",
    "This was the best decision tree model that I sent to tester.py for testing.\n",
    "\n",
    "    ['poi', 'salary', 'total_payments', 'exercised_stock_options', 'restricted_stock', 'expenses', 'director_fees', 'deferred_income', 'from_poi_to_this_person_fraction', 'from_this_person_to_poi_fraction', 'poi_email_reciept_interaction']\n",
    "    \n",
    "    GridSearchCV(cv=None, error_score='raise',\n",
    "           estimator=Pipeline(steps=[('pca', PCA(copy=True, n_components=None, whiten=False)), ('dt', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
    "                max_features=None, max_leaf_nodes=None, min_samples_leaf=3,\n",
    "                min_samples_split=10, min_weight_fraction_leaf=0.0,\n",
    "                presort=False, random_state=None, splitter='best'))]),\n",
    "           fit_params={}, iid=True, n_jobs=1, param_grid={},\n",
    "           pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)\n",
    "        \n",
    "        Accuracy: 0.84427\tPrecision: 0.40289\tRecall: 0.34850\tF1: 0.37373\tF2: 0.35817\n",
    "        Total predictions: 15000\tTrue positives:  697\tFalse positives: 1033\tFalse negatives: 1303\tTrue negatives: 11967\n",
    "\n",
    "Tuning a model is important because the parameters you feed into classifiers (SelectKBest, PCA, DecisionTree etc.) will all affect how your final model generalizes on unseen data. SciKit Learn has several tools to help us tune, e.g. Cross Validation and GridSearchCV. Usually when tuning and validating our model, we started by breaking up our data into three partitions: Training, Validating and Testing sets. Models are generally fit to training data and then tuned and adjusted on a validating data set. When you have achieved a desired performance metric you then test your final model on unseen testing data. This described method, cross validation, is a good rule to follow because it ensures that you are not overfitting your model by testing it on data that it was trained on. Within the cross validation methodology there exists several data set dependant formats e.g. simple splitting, shuffle splitting and so on. \n",
    "\n",
    "GirdSearchCV is a tool provided by SciKit Learn that allows a user to test out a model with different configurations of parameters. For Example PCA has a parameter, n_components=(1,2...n). Where n is the number of dimensional spaces PCA reduces the features too. Instead of creating several models all with different n values, we pass PCA to the GridSearchCV() . Then we pass a dictionary containing all the different values of n we wish to try. Grid search runs all combinations for us and reports back which model was the optimal. GridSearchCV also uses a 3 k-fold cross validation to test models. GridSearchCV allows us to more easily tune the parameters of a model, this is called hyperparameter tuning. The downside is when you tune several parameters and several features all part of a model. This can take a long time.\n",
    "\n",
    "For this project I choose to use GridSearchCV to perform parameter tuning as well as cross validation. I did not want to split up my data into separate training/validation/testing sets for two reasons.\n",
    "\n",
    "This is a small data set to begin with only ~140 observations.\n",
    "There is a severe class imbalance. There are 18 POIs, the rest would be classified as non-POIs.\n",
    "I do not want to risk randomly separating data and losing a portion of POIs to a test set. Because we would lose valuable training data. On the flip side we would also have no POIs to test on in that scenario. So this is kinda of a zero win situation. Therefore I recommend using GridSearch’s cross validation ability to perform a stratified shuffle split cross validation on the model. Code below:\n",
    "\n",
    "    folds = 1000\n",
    "    cv = StratifiedShuffleSplit(labels_df, folds, random_state=42)\n",
    "    clf = GridSearchCV(estimator, param_dict, cv=cv)\n",
    "\n",
    "Additionally this matches Udacity’s tester.py file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate and Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usage of Evaluation Metrics\n",
    "\n",
    "While accuracy can be a good general predictor of model performance, it is not the only evaluation criteria we can and should use. Precision, Recall and F1 scores are available as well. Precision and Recall are especially well suited for this data set. Due to the severe class imbalance of POIs vs non-POIs our model could classify every observation as not a POI and still receive a high accuracy score ~85%. This would be the absolute worst model we could make. \n",
    "\n",
    "Let’s look at using Precision, Recall and F1 score.\n",
    "\n",
    "Precision (score between 0.0-1.0) can be thought of as **exactness**. Specifically a high score tells us that we have less false positives. In other words the less innocent people classified as a POI the higher our precision score.  The equation is below:\n",
    "\n",
    "##### *Precision* = (*TP*) / (*TP* + *FP*)\n",
    "\n",
    "Where:\n",
    "- TP = True Positive (a POI correctly classified as POI)\n",
    "- FP = False Positive (a non-POI incorrectly classified as a POI)\n",
    "\n",
    "Recall can be thought of as **completeness**. Specifically a high Recall score tells us that we have less false negatives. In other words the less POIs classified as innocent people the higher our recall score. Equation below:\n",
    "\n",
    "##### *Recall* = (*TP*) / (*TP* + *FN*)\n",
    "\n",
    "Where:\n",
    "- TP = True Positive (a POI correctly classified as POI)\n",
    "- FN = False Negative (a POI incorrectly classified as a non-POI)\n",
    "\n",
    "***F1 Scores*** provide you with a value that determines accuracy of a model with respect to precision and recall. The higher the F1 score the higher precision and recall  you have. Equation below:\n",
    "\n",
    "##### *F1* = (*precision* \\* *recall*) / (*precision* + *recall*)\n",
    "\n",
    "\n",
    "For this prediction algorithm I would argue that we care more about Recall. Because were are concerned with identifying POIs for further investigation. Therefore we rather get all POIs even if that means investigating some innocent people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Strategy\n",
    "\n",
    "Validation is very important in machine learning. For example validating a classifier can reduce over fitting on never before seen data, ensuring generalizability. This mindset goes back to the bias-variance tradeoff. Models may be more biased (think linear regression) or variance (think SVM). Often an overly biased model  does not meet the complexities of the real world and we thus underfit. On the flip side a high variance model that overfits the data may produce great results, but in the real world will often be lackluster. Thus a balance is needed. Methods like cross validation and all of its implementations helps alleviate this issue, but employing various training, validating and testing sets to ensure models train on one set of data and are tested on a pure untouched set of data as well, usually smaller than the fit. \n",
    "\n",
    "This methodology can run into issues though when your data set is not very large, like the Enron set. Or classes are imbalanced, like in the Enron set (18 POIs vs. over a hundred non-POIs). To help remedy these issues we can employ strategies such as StratefiedShuffeSplit to partition our dataset and test, without having to permanently partition a training and testing set. \n",
    "\n",
    "Another viable CrossValidation strategy would be to perform a LeaveOneOut. This validation strategy can be computationally costly because the number of training/testing sets is equal to the number of data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm Performance\n",
    "\n",
    "The score for my final algorithm was:\n",
    "\n",
    "    Accuracy: 0.84427\tPrecision: 0.40289\tRecall: 0.34850\tF1: 0.37373\n",
    "    True positives:  697\tFalse positives: 1033\tFalse negatives: 1303\tTrue negatives: 11967\n",
    "    \n",
    "The algorithm was:\n",
    "\n",
    "    GridSearchCV(cv=None, error_score='raise',\n",
    "           estimator=Pipeline(steps=[('pca', PCA(copy=True, n_components=None, whiten=False)), ('dt', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
    "                max_features=None, max_leaf_nodes=None, min_samples_leaf=3,\n",
    "                min_samples_split=10, min_weight_fraction_leaf=0.0,\n",
    "                presort=False, random_state=None, splitter='best'))]),\n",
    "           fit_params={}, iid=True, n_jobs=1, param_grid={},\n",
    "           pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)\n",
    "           \n",
    "And feature list:\n",
    "\n",
    "    ['poi', 'salary', 'total_payments', 'exercised_stock_options', 'restricted_stock', 'expenses', 'director_fees', 'deferred_income', 'from_poi_to_this_person_fraction', 'from_this_person_to_poi_fraction', 'poi_email_reciept_interaction']\n",
    "\n",
    "I was able to get a decent precision score; however, I had trouble getting recall past the .40 mark. Attempting to reproduce results with a pipeline() and GridSearchCV() also gave me some trouble. However, I was able to produce a simple model eith precision and recall both above .30 using manual manipulation techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. https://www.oreilly.com/learning/handling-missing-data imputation\n",
    "2. http://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues PCA\n",
    "3. Python Machine Learning by Raschka\n",
    "4. ISL by Hastie et al.\n",
    "5. https://discussions.udacity.com/t/using-pipeline-precision-recall-have-been-decreased/45992/5\n",
    "6. SkLearn documentation\n",
    "7. PANDAS documentation\n",
    "8. Seaborn documentation\n",
    "9. MatPlotLib documentation\n",
    "10. http://stats.stackexchange.com/questions/53/pca-on-correlation-or-covariance\n",
    "11. Many more stack exachage, cross validated, and udacity forum posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
