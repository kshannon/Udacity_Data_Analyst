{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kyle Shannon 1/1/16 - Project 3 - Data Wrangling with Mongodb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I choose to download city data of Seattle, Washington from <insert URL Here...>. While I live in San Diego and have never lived in Seattle, I choose this dataset because my girlfriend and have begun planning a trip to Seattle. I figuered this project would be helpful in learning at a high level a bit more about Seattle and what the city has to offer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description of Files in Folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **project3_dataWrangling.py** This is the main python file that creates the mongodb, loads the seattle_washington.osm file, parses it, and inserts it into the database. \n",
    "- **db_query.py** queries the database. There is one pipeline in the file; however, below the code in comments are all the pipelines I wrote with sample outputs. This way, I can simply copy paste a pipeline if I wish to run an older query.\n",
    "- **xml_decimate.py** The meat of this code was provided by the course instructor as a way of decimating a .osm file. This was useful for testing purposes as it is much faster combing through a tenth of the data.\n",
    "- **Helper scripts .py files:** There are several scripts in here that I used along with Grep in the terminal to gather facts about the .osm file. I primarily ran these .py files and grepped through the decimated .osm file as it took about a .10th of the time. This helped me learn more about the data I was about to parse and what type of exceptions I might run inot and encounter. This helped me write the main parsing function(s). \n",
    "- **example.osm** This dataset was made from the chicago sample used in lessons. This really just privided me with an instantaneous spot check when I was wrting code.\n",
    "- **sample_seattle.osm** This osm file is the decimated dataset from seattle_washington.osm. I used this dataset to run my python scripts and grep commands against. It provided me a good approximation of what I might run into when parsing the large .osm file. (this file is not included in the folder due to size, but can be downloaded from here: <insert link>)\n",
    "- **seattle_washington.osm** The large ~1.5gb .osm file for Seattle. (this file is not included in the folder due to size, but can be downloaded from here: <insert link>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach Taken During Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decimating the large .osm file was first. Leaving me with an xml file ~150.0 mb. Using code from lesson 6 exercises I wrote several scripts to find info about the data. E.g. unique zipcodes, country codes and road types. Combining this with Grep e.g.: \n",
    "    \n",
    "    Kyles-MBP:project 3 kyleshannon$ grep -e -A 10 -B 10 mexican sample_seattle.osm | less \n",
    "\n",
    "which returns a line of xml data containing the string 'mexican' along with the 10 lnes before it and after it. This combination provided me a time effective way of learning about the data set before writing my main parsing script. The python file that handles parsing the data also handles creating and inserting documents into a mongodb. E.g.: \n",
    "    \n",
    "    def process_osm(file_in):\n",
    "        with open(file_in) as file: #opens and closes the xml file.\n",
    "            for _, element in ET.iterparse(file): #iterating through .osm file\n",
    "                el = shape_data(element) #parsing data elements one by one\n",
    "                if el:\n",
    "                    #pprint.pprint(el)\n",
    "                    way_node_collection.insert(el) #writing parsed data into mongodb\n",
    "\n",
    "This script took ~20 min to run on a ~1.5gb xml file. After that my mongodb was created and the documents were already inserted into it. I could then begin to query the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems Encountered in your Map. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the pre-query pahse of the project there were several issues that I decided the let the parser handle. This included wrong country codes, city names, zip codes, standardizing road type names, weird address data and what I call Source scope creep. These are discussed in detail bellow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Country Codes/City Names/Postcodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because Seattle is so close to Canada, I wanted to make sure no Canadian data snuck through. Running:\n",
    "\n",
    "    grep -e country_code sample_seattle.osm | less\n",
    "\n",
    "returned a list of country codes in the dataset, I noticed several Canadian codes. Therefore I wrote a placeholder function to check \n",
    "\n",
    "    country code.lower() == 'us'. \n",
    "    \n",
    "The same process occured for city names. I noticed several city names (Tacoma, Kirkland and so on.) These may be in the county of Seattle (King's County), but they are not part of the city of Seattle. Therefore I decided not to include them.\n",
    "\n",
    "A final check included postcodes (zipcodes). I went online and found the postcodes associated with the city of Seattle. I used a python script to check if any postcodes not in this list were in the sample_seattle.osm dataset:\n",
    "\n",
    "    import xml.etree.cElementTree as ET\n",
    "    import pprint\n",
    "\n",
    "    #zipcodes sourced from http://zipcode.org/city/WA/SEATTLE\n",
    "    wa_zipcodes = ['98101', ... '98198']\n",
    "\n",
    "    def get_zipcode(element):\n",
    "        return element.get('postcode')\n",
    "\n",
    "    def unique_users(filename):\n",
    "        erras = set()\n",
    "        count = 0\n",
    "        for _, element in ET.iterparse(filename):\n",
    "            if count = 10:\n",
    "                break\n",
    "            if get_zipcode(element) in wa_zipcodes:\n",
    "                continue\n",
    "            erras.add(get_zipcode(element))\n",
    "            count += 1\n",
    "        print len(erras)\n",
    "\n",
    "    xml_file = \"sample_seattle.osm\"\n",
    "    unique_users(xml_file)\n",
    "\n",
    "Sure enough there were other postcodes. This became another function in my main parser. How I handled catching data I did not want to insert, was by setting a flag at the top of my main data parsing function: is_valid = True. If any of these cases were found, then the flag would become false and at the end of the function the parsed data would not be returned, thus not be inserted into mongo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unifying Road Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, I used grep along with a simple script to collect info about the various road types. Then I created a dict called MAPPING which contained key/value pairs. E.g. \n",
    "\n",
    "    {'st':'Street', 'st.':'Street'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weird Addresses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addresses provided a bit of a headache due to its insidious nature. For example some addresses were easy to flag as they followed an: 'addr:xyz'; however some had multiple colons ':' in them. When addr's contained multiple colons they appeared to be duplicates, so I decided to ignore them and not include them in the database and continue along with the for loop:\n",
    "\n",
    "    if key.startswith('addr:'):\n",
    "\t\tif ':' in key[5:]:\n",
    "            continue\n",
    "\n",
    "Some entries had a flag in them called 'fixme'. I decided to throw out all of these entries completely. If there was one issue with parts of the data, then how could I trust the rest of the document being written. Code below:\n",
    "\n",
    "\tif key.lower() == 'fixme':\n",
    "\t\tis_valid = False\n",
    "\t\tcontinue\n",
    "It is during this part of the parsing that I check if the address is in Seattle by passing the element's key into the country, city and postcode functions: \n",
    "\n",
    "\tif addr_key.lower() == 'country':\n",
    "\t\tis_valid = is_valid_country(val)\n",
    "\t\t\t\t\t\n",
    "    if addr_key.lower() == 'city':\n",
    "\t\tis_valid = is_valid_city(val)\n",
    "\t\t\t\t\t\n",
    "\tif addr_key.lower() == 'postcode':\n",
    "\t\tis_valid = is_valid_postcode(val)\n",
    "\t\t\t\t\n",
    "\tif not is_valid:\n",
    "\t\tcontinue\n",
    "        \n",
    "Another check I perform is that an address was applied to the empty address dictionary. If not then I do not want to create an empty address, that would create an empty address document in the database, which we do not want:\n",
    "\n",
    "    if len(address) > 0:\n",
    "        node['address'] = address\n",
    "\n",
    "Once these addresses check out then I wrote them into the address dictionary and continue on with the for loop through the element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Source Scope Creep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One interesting I found was what I am calling source scope creep. What occurs is when there was a source 'tiger_import' there was accompanying 'tiger:xyz' duplicates for preexisting 'addr:xyz' fields. I was thinking about removing all of these dupicates, but in the end I decided to keep the tiger data as it did not interfere with my data and it might be useful as the tiger data did include other fields that were unique. But I call it scope creep because the source which is Tiger is finding its way into fields like postcode and county. Where it should not be. \n",
    "\n",
    "    \n",
    "    <tag k=\"tiger:source\" v=\"tiger_import_dch_v0.6_20070830\" />\n",
    "    <tag k=\"tiger:county\" v=\"Clallam, WA\" />\n",
    "    <tag k=\"addr:county\" v=\"Clallam, WA\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Ideas About the Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources/Resources Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
